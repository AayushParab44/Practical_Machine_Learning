{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc81fbce-c1ed-458d-bc51-2c7ed94da45d",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a34a4e-39c4-4ef7-bc24-9b2764a3de9e",
   "metadata": {},
   "source": [
    "The vertical split segregates - x and y where x is the predictors and y is the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d4023-42d1-4df4-b109-bf0aa0ff5909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "279c6c8d-e5a8-4270-a9e5-0aff70b00626",
   "metadata": {},
   "source": [
    "# Lexical Ambiguity - \n",
    "- word has different meanings.<br>\n",
    "- word being understood in more than 1 way.<br>\n",
    "- Machine has difficulty in interpreting these.\n",
    "- Ex.- Bank- 1. Financial institution, 2. River bank "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd421c-3ba8-42b7-9fbd-d90e376f374e",
   "metadata": {},
   "source": [
    "# Syntax Ambiguity - syntactic ambiguity\n",
    "- when a sentence can be parsed in different ways then this type of ambiguity is called syntactic ambiguity.<br>\n",
    "- ex. The man saw a girl with a telescope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194f687-89b1-4ccf-ad9f-07b740b1c8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fd979ca-63fe-4c65-adc0-de7a9a680c55",
   "metadata": {},
   "source": [
    "# Symantic ambiguity\n",
    "- when the meaning of the words themselves can be misinterpreted \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b871d-5b24-463a-9329-4774ac96e3d7",
   "metadata": {},
   "source": [
    "\n",
    "# Anaphoric ambiguity \n",
    "- arises due to the use of anaphore in the sentence\n",
    "- Ex- The horse run up the hill. It was too steep . It soon got tired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960caf01-9b5a-47d9-b974-cbec4b90896e",
   "metadata": {},
   "source": [
    "# Pragmatic ambiguity\n",
    "- When the statement is not specific or the context of the phrase gives multiple interpretations.\n",
    "- Ex. I love you too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a14502-9928-45f8-9076-54f5f068da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71c98c-27b2-4a4c-8a38-180759f12eb9",
   "metadata": {},
   "source": [
    "# Cleaning Data\n",
    "- convert data into lowercase\n",
    "- remove punctuations - .,,'',\"\",!,?,@,$,-\n",
    "- Remove stopwords - I, a ,an"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6efcd-746c-45e2-ba69-44108ceca865",
   "metadata": {},
   "source": [
    "- nltk library - natural language toolkit used in nlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7770c-431d-484c-ad1b-0d828bcbb913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4ef3fc-4d6e-4112-87b1-9ad85de1ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import string\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64af8c5a-74e5-425e-9180-5a869cb91e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading nltk data(first time only)-\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger_eng') #supervised machine learning model for POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca000d-06f3-4174-a69a-317b504ac95a",
   "metadata": {},
   "source": [
    "# Tokenization \n",
    "- splitting a phrase/sentence/paragraph or entire text into smaller units(tokens) such as words is called as tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f97ac6b-d3d8-48ea-b47f-b24804711146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='The quick brown fox jumped over the lazy dog!'\n",
    "\n",
    "# Tokenize-\n",
    "tokens=word_tokenize(text.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a281091c-b8d0-452c-bb41-676e4b4d0f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "062e236e-645a-49ff-9625-e5bb58cef46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e1bc683-9965-4919-828e-f5720656be56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(stopwords.words('english')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22801303-48cb-4413-aba7-6e44264654f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa1549e8-58ca-4ba8-9bbc-fca8a2caeeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "tokens=[word for word in tokens if word not in string.punctuation]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6a875bf-b68c-43eb-9190-5ead47343de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "tokens=[word for word in tokens if word not in stopwords.words('english')]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55763d2d-38a0-49d3-bd8d-917e34372de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46865670-7c1c-4f55-9cc7-da384c96c907",
   "metadata": {},
   "source": [
    "# Stemming \n",
    "- reducing the word to their stem(base word) is called stemming.\n",
    "- ex- moving,move,moved - move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee92dc-a373-4340-ac63-1f11fcc7d31a",
   "metadata": {},
   "source": [
    "- It stems the word but sometimes the context of the word gets lost.\n",
    "- ex.- studies -> studi\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8dee35-31a2-489a-982c-d5601d811b4b",
   "metadata": {},
   "source": [
    "# About Lemmatization-\n",
    "- lemmatization is a concept which solves this problem.\n",
    "- and the base word will have some meaning.\n",
    "- lemmatization has a dictionary and will look for a word similiar to the stemmed word, and will eventually give the correct word which has that meaning\n",
    "- ex. - studies -> study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f58244-64e2-4b17-ab0d-2c5bdc168a2e",
   "metadata": {},
   "source": [
    "# Difference between stemming and lemmatization-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abece97-ad15-45e1-83d3-bacc033b1cfe",
   "metadata": {},
   "source": [
    "| Stemming | Lemmatization |\n",
    "| --- | --- |\n",
    "| - Stemming is faster. <br> - It is a rule-based approach (just remove the end part to stem the word). <br> - Less accuracy in stemming. <br> - Stemming is used in case of spam detection. | - Lemmatization is slower than stemming. <br> - It is a dictionary-based approach. <br> - Accuracy is more. <br> - Lemmatization is used for text summarizing, question and answers. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da355266-aac9-4048-b53d-a4dc0ed58025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54237b76-a85e-46af-9f2e-646d68640fa3",
   "metadata": {},
   "source": [
    "# Stemming-\n",
    "- Stemming is faster.\n",
    "- It is a rule based approach(just remove the end part to stem the word).\n",
    "- Less accuracy in stemming.\n",
    "- Stemming is used in case of spam detection.\n",
    "\n",
    "# Lemmatization-\n",
    "- Lemmatization is slower than stemming.\n",
    "- It is a dictionary based approach.\n",
    "- Accuracy is more.\n",
    "- Lemmatization is used for text summarizing, question and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0589b7-bb19-4c55-8627-2da7fbca3d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88a879e9-f263-43f1-99f4-dbff8dc71d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer  #PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "stemmed_words=[stemmer.stem(word) for word in tokens]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e7b66-ab41-417d-9e2a-b9bce0f401b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41451b77-915f-48a5-a0eb-6d7990efae4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()   #Lemmatizer\n",
    "tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7445f79a-2f4f-49bc-ba70-4af9fc07d7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d0ddc3-996f-4e1c-a1f8-82353d8f5621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Hello', ',', 'All', '!', 'This', 'is', 'an', 'NLP', 'example', '.', 'I', 'love', 'learning', 'about', 'transformers', ',', 'attention', 'mechanisms', ',', 'and', 'embeddings']\n",
      "\n",
      "\n",
      "Lowercased Tokens: ['hello', ',', 'all', '!', 'this', 'is', 'an', 'nlp', 'example', '.', 'i', 'love', 'learning', 'about', 'transformers', ',', 'attention', 'mechanisms', ',', 'and', 'embeddings']\n",
      "\n",
      "\n",
      "Without punctuation: ['hello', 'all', 'this', 'is', 'an', 'nlp', 'example', 'i', 'love', 'learning', 'about', 'transformers', 'attention', 'mechanisms', 'and', 'embeddings']\n",
      "\n",
      "\n",
      "Without stopwords: ['hello', 'nlp', 'example', 'love', 'learning', 'transformers', 'attention', 'mechanisms', 'embeddings']\n",
      "\n",
      "\n",
      "Lemmatized Tokens: ['hello', 'nlp', 'example', 'love', 'learning', 'transformer', 'attention', 'mechanism', 'embeddings']\n",
      "\n",
      "\n",
      "Stemmed tokens: ['hello', 'nlp', 'exampl', 'love', 'learn', 'transform', 'attent', 'mechan', 'embed']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Another example-\n",
    "# pos- parts of speech\n",
    "\n",
    "#Sample text\n",
    "text='Hello, All! This is an NLP example. I love learning about transformers, attention mechanisms, and embeddings'\n",
    "\n",
    "#step1: Tokenization:\n",
    "tokens=word_tokenize(text)\n",
    "print(\"Tokens:\",tokens)\n",
    "print('\\n')\n",
    "\n",
    "#step 2: lowercasing\n",
    "tokens=[word.lower() for word in tokens]\n",
    "print('Lowercased Tokens:',tokens)\n",
    "print('\\n')\n",
    "#step 3: Remove punctuation\n",
    "tokens=[word for word in tokens if word not in string.punctuation]\n",
    "print('Without punctuation:',tokens)\n",
    "print('\\n')\n",
    "\n",
    "#step 4: Remove stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "tokens=[word for word in tokens if word not in stop_words]\n",
    "print('Without stopwords:',tokens)\n",
    "print('\\n')\n",
    "\n",
    "#step 5 : Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
    "print('Lemmatized Tokens:',lemmatized_tokens)\n",
    "print('\\n')\n",
    "\n",
    "#step 6 : Stemming\n",
    "stemmer=PorterStemmer()\n",
    "stemmed_words=[stemmer.stem(word) for word in lemmatized_tokens]\n",
    "print('Stemmed tokens:',stemmed_words)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ef6ebd-d6bf-43ba-ab8a-11881b06b0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'nlp',\n",
       " 'example',\n",
       " 'love',\n",
       " 'learning',\n",
       " 'transformers',\n",
       " 'attention',\n",
       " 'mechanisms',\n",
       " 'embeddings']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ea81721-e12b-46f3-bf6f-16ed9865be28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('hello', 'NN'), ('nlp', 'JJ'), ('example', 'NN'), ('love', 'IN'), ('learning', 'VBG'), ('transformers', 'NNS'), ('attention', 'NN'), ('mechanisms', 'NNS'), ('embeddings', 'NNS')]\n",
      "\n",
      "\n",
      "Bigrams: [('hello', 'nlp'), ('nlp', 'example'), ('example', 'love'), ('love', 'learning'), ('learning', 'transformers'), ('transformers', 'attention'), ('attention', 'mechanisms'), ('mechanisms', 'embeddings')]\n"
     ]
    }
   ],
   "source": [
    "#step 7 : POS TAGGING-\n",
    "pos_tags=pos_tag(tokens)\n",
    "print('POS Tags:',pos_tags)\n",
    "print('\\n')\n",
    "\n",
    "#step 8: Generate N-grams\n",
    "bigrams=list(ngrams(tokens,2))\n",
    "print('Bigrams:',bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960094f-ac78-44d1-ad74-e779fa8c2b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8988f-8a60-406a-b1d2-51cf7fa5851a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be5b82a1-474b-435b-9fed-61a2f0ae892d",
   "metadata": {},
   "source": [
    "# POS tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53784c1-45f5-4f57-9245-7510af88de5b",
   "metadata": {},
   "source": [
    "pos_tag->meaning -> example\n",
    "1. NN ->Nouns singular->\n",
    "2. NNS-> Nouns Plural ->\n",
    "3. VB-> Verb base form-> run,eat,read\n",
    "4. VBD-> Verb in the past tense -> ate, ran,\n",
    "5. VBG -> ing form of verb -> running, sleeping\n",
    "6. VBN -> verb past participle -> eaten,\n",
    "7. JJ-> adjectives -> beautiful\n",
    "8. RB -> adverb -> slowly, quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5adb8cd4-0ac6-43ab-b002-5667f51d0e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordnet.NOUN\n",
    "# wordnet.VERB\n",
    "# wordnet.ADV\n",
    "# wordnet.ADJ\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521566f-3889-443e-ad96-cb8b21cceb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a94a5da1-1c2b-4bf2-aa63-575de483951f",
   "metadata": {},
   "source": [
    "# Bag of words -\n",
    "- is a text representation technique where the text is represented as collection of words(bag) where the grammar annd the word order is not taken into account but the frequency of the words matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96403a-3272-4654-a151-afa3d01a1c6c",
   "metadata": {},
   "source": [
    "Each sentence is considered as a document<br>\n",
    "vocabulary->unique words<br>\n",
    "- Tokenization\n",
    "- Vocabulary\n",
    "- Vectorisation(if a particualar word is there it is given value 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ed4106f-5417-4832-9210-5a785c726bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 6 stored elements and shape (2, 4)>\n",
      "  Coords\tValues\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 0)\t1\n",
      "[[0 1 1 1]\n",
      " [1 1 0 1]]\n",
      "['fun' 'learning' 'love' 'machine']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#sample documents\n",
    "docs=['I love machine learning','Machine learning is fun']\n",
    "\n",
    "#initialize count vectorizer\n",
    "vectorizer=CountVectorizer(lowercase=True,stop_words='english')\n",
    "\n",
    "#fit and transform the documents into a bag of words-\n",
    "X=vectorizer.fit_transform(docs)\n",
    "print(X)\n",
    "\n",
    "# convert the result to an array\n",
    "print(X.toarray())\n",
    "\n",
    "#Display the vocabulary(features)-\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3131101-0348-4f91-ba32-5c20724a0dc9",
   "metadata": {},
   "source": [
    "sparse matrix has higher computation speed than dense matrix, so we convert dense matrix to sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77309e5-c7a8-457a-86f0-3680e010410e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ffd985-984e-489d-ae82-4b825af8e36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "659a4b4c-e63f-465f-ba76-b471cba3272f",
   "metadata": {},
   "source": [
    "# Limitations of bag of words\n",
    "- hLoss of context\n",
    "- High vector size(high dimensionality or vector size)\n",
    "- There is no symantic meaning.\"\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614411c-1d8b-4a08-b59b-10eadd4e5ae8",
   "metadata": {},
   "source": [
    "term freq - no. o reperition of word kns a srence==========/Total number of words in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca25458-b21d-4b11-b492-6b15f19e9941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf914699-e3b7-4e3b-b944-89c21989ccc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c41c5e81-fc68-4dd3-8640-c0fbdce4e949",
   "metadata": {},
   "source": [
    "TF-IDF -Term Frequency - Inverse Document frequency\n",
    "- This is a statistical method to evaluate the importance of a word in a documeny, by calculating a score for each of the word\n",
    "- It is generally used for information retrieval and summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85784a3-0d0a-4ef5-9c92-b4d0d0b2a311",
   "metadata": {},
   "source": [
    "idf= number of sentences in numerator / number of sentences containing the word\n",
    "\n",
    "Rare words will have high idf values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e7f45-5a9f-4160-acbd-2bed4a3b254f",
   "metadata": {},
   "source": [
    "Idf measures how unique a word is across all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5489dc1-0af3-4898-8b83-c7487b5f9dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e3f6fb-b364-455d-943b-503c5907eb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2246d94-761e-4dc9-83e3-dc4e04d5ff3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "495d8289-11ee-4dd4-ac3e-64a502f773cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f72c2a63-ac73-47f9-a9fa-58dd08c7fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=['The cat sat on the mat','The dog barked at the cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "387af723-cf18-42bf-a9cc-198afca4c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "608821b4-2961-4114-a738-8d58dc2eec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 10 stored elements and shape (2, 8)>\n",
      "  Coords\tValues\n",
      "  (0, 7)\t0.6050614264813995\n",
      "  (0, 2)\t0.30253071324069974\n",
      "  (0, 6)\t0.42519636159088015\n",
      "  (0, 5)\t0.42519636159088015\n",
      "  (0, 4)\t0.42519636159088015\n",
      "  (1, 7)\t0.6050614264813995\n",
      "  (1, 2)\t0.30253071324069974\n",
      "  (1, 3)\t0.42519636159088015\n",
      "  (1, 1)\t0.42519636159088015\n",
      "  (1, 0)\t0.42519636159088015\n"
     ]
    }
   ],
   "source": [
    "x=vectorizer.fit_transform(documents)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36388f6f-3fba-4672-9b99-42bb0223acaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['at' 'barked' 'cat' 'dog' 'mat' 'on' 'sat' 'the']\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary:',vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e0df45f-5ae5-47d1-80f3-13561e0c172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF MAtrix:\n",
      " [[0.         0.         0.30253071 0.         0.42519636 0.42519636\n",
      "  0.42519636 0.60506143]\n",
      " [0.42519636 0.42519636 0.30253071 0.42519636 0.         0.\n",
      "  0.         0.60506143]]\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF MAtrix:\\n\",x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74175f05-b662-404b-981d-e8bffea53c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404802a-2127-4f2b-a1e6-1a1d1d84d6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03094960-02be-4ac7-b1b2-5c941a9fe08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed0029c7-25f6-4fff-8665-0476e07072ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=['The cat sat on the mat','The dog barked at the cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "191ce2e8-b550-4f5c-89a8-72c375b3d765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 20 stored elements and shape (2, 17)>\n",
      "  Coords\tValues\n",
      "  (0, 13)\t0.44915675016915907\n",
      "  (0, 4)\t0.22457837508457953\n",
      "  (0, 11)\t0.31563707021700443\n",
      "  (0, 9)\t0.31563707021700443\n",
      "  (0, 8)\t0.31563707021700443\n",
      "  (0, 14)\t0.22457837508457953\n",
      "  (0, 5)\t0.31563707021700443\n",
      "  (0, 12)\t0.31563707021700443\n",
      "  (0, 10)\t0.31563707021700443\n",
      "  (0, 16)\t0.31563707021700443\n",
      "  (1, 13)\t0.44915675016915907\n",
      "  (1, 4)\t0.22457837508457953\n",
      "  (1, 14)\t0.22457837508457953\n",
      "  (1, 6)\t0.31563707021700443\n",
      "  (1, 2)\t0.31563707021700443\n",
      "  (1, 0)\t0.31563707021700443\n",
      "  (1, 15)\t0.31563707021700443\n",
      "  (1, 7)\t0.31563707021700443\n",
      "  (1, 3)\t0.31563707021700443\n",
      "  (1, 1)\t0.31563707021700443\n",
      "\n",
      "\n",
      "Features (N-Grams): ['at' 'at the' 'barked' 'barked at' 'cat' 'cat sat' 'dog' 'dog barked'\n",
      " 'mat' 'on' 'on the' 'sat' 'sat on' 'the' 'the cat' 'the dog' 'the mat']\n",
      "\n",
      "\n",
      "TF-IDf matrix:\n",
      " [[0.         0.         0.         0.         0.22457838 0.31563707\n",
      "  0.         0.         0.31563707 0.31563707 0.31563707 0.31563707\n",
      "  0.31563707 0.44915675 0.22457838 0.         0.31563707]\n",
      " [0.31563707 0.31563707 0.31563707 0.31563707 0.22457838 0.\n",
      "  0.31563707 0.31563707 0.         0.         0.         0.\n",
      "  0.         0.44915675 0.22457838 0.31563707 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#generate TFIDF features with N-Grams(unigrams,bigrams)\n",
    "vectorizer=TfidfVectorizer(ngram_range=(1,2))\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "print(X)\n",
    "print('\\n')\n",
    "#Print feature names(vocabulary)\n",
    "print('Features (N-Grams):',vectorizer.get_feature_names_out())\n",
    "print('\\n')\n",
    "\n",
    "#print tf-idf matrix-\n",
    "print(\"TF-IDf matrix:\\n\",X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407ccba-0294-4de3-a0f7-17e9056b077c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a24d43-1ab3-482b-922c-24d79f97bb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec036be1-2ea2-4e46-9d15-d78c8cc03c5a",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "- are dense vector representations of words generated through neural networks.\n",
    "- Each word is mapped to a high dimensional vector space where the relationships between the words are preserved.\n",
    "\n",
    "# Advantages of word embeddings-\n",
    "- symantic as well as syntactic meaning both are preserved.\n",
    "- embeddings trained on large corpus understand the word relationship.\n",
    "- accuracy is improved.\n",
    "\n",
    "\n",
    "Different word embedding models which are available- Word2Vec(pretrained model by google),Glove(pretrained model by stanford),FastText(pretrained model by facebook)\n",
    "\n",
    "2 Architectures in word2vec -\n",
    "- CBow - continuous bag of words\n",
    "- Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b71bb-8421-4d61-ba99-d91f6355573f",
   "metadata": {},
   "source": [
    "Glove - Global vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a91c1-7091-4f6c-ac74-3ae977e0e4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854859c6-6b9f-4c5f-aa29-90f3c5621187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ef4c2-65e2-457a-8de1-c1ec933721ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ff779-492e-47cd-9957-0e6b63026ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40037cce-677b-4b8f-b732-3c548d7ec03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cda984-035c-4819-96fb-93ca547c740a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
