{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc81fbce-c1ed-458d-bc51-2c7ed94da45d",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a34a4e-39c4-4ef7-bc24-9b2764a3de9e",
   "metadata": {},
   "source": [
    "The vertical split segregates - x and y where x is the predictors and y is the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d4023-42d1-4df4-b109-bf0aa0ff5909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "279c6c8d-e5a8-4270-a9e5-0aff70b00626",
   "metadata": {},
   "source": [
    "# Lexical Ambiguity - \n",
    "- word has different meanings.<br>\n",
    "- word being understood in more than 1 way.<br>\n",
    "- Machine has difficulty in interpreting these.\n",
    "- Ex.- Bank- 1. Financial institution, 2. River bank "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd421c-3ba8-42b7-9fbd-d90e376f374e",
   "metadata": {},
   "source": [
    "# Syntax Ambiguity - syntactic ambiguity\n",
    "- when a sentence can be parsed in different ways then this type of ambiguity is called syntactic ambiguity.<br>\n",
    "- ex. The man saw a girl with a telescope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194f687-89b1-4ccf-ad9f-07b740b1c8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fd979ca-63fe-4c65-adc0-de7a9a680c55",
   "metadata": {},
   "source": [
    "# Symantic ambiguity\n",
    "- when the meaning of the words themselves can be misinterpreted \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b871d-5b24-463a-9329-4774ac96e3d7",
   "metadata": {},
   "source": [
    "\n",
    "# Anaphoric ambiguity \n",
    "- arises due to the use of anaphore in the sentence\n",
    "- Ex- The horse run up the hill. It was too steep . It soon got tired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960caf01-9b5a-47d9-b974-cbec4b90896e",
   "metadata": {},
   "source": [
    "# Pragmatic ambiguity\n",
    "- When the statement is not specific or the context of the phrase gives multiple interpretations.\n",
    "- Ex. I love you too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a14502-9928-45f8-9076-54f5f068da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71c98c-27b2-4a4c-8a38-180759f12eb9",
   "metadata": {},
   "source": [
    "# Cleaning Data\n",
    "- convert data into lowercase\n",
    "- remove punctuations - .,,'',\"\",!,?,@,$,-\n",
    "- Remove stopwords - I, a ,an"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6efcd-746c-45e2-ba69-44108ceca865",
   "metadata": {},
   "source": [
    "- nltk library - natural language toolkit used in nlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7770c-431d-484c-ad1b-0d828bcbb913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4ef3fc-4d6e-4112-87b1-9ad85de1ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64af8c5a-74e5-425e-9180-5a869cb91e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\AAYUSH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AAYUSH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AAYUSH\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloading nltk data(first time only)-\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca000d-06f3-4174-a69a-317b504ac95a",
   "metadata": {},
   "source": [
    "# Tokenization \n",
    "- splitting a phrase/sentence/paragraph or entire text into smaller units(tokens) such as words is called as tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f97ac6b-d3d8-48ea-b47f-b24804711146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='The quick brown fox jumped over the lazy dog!'\n",
    "\n",
    "# Tokenize-\n",
    "tokens=word_tokenize(text.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a281091c-b8d0-452c-bb41-676e4b4d0f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "062e236e-645a-49ff-9625-e5bb58cef46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e1bc683-9965-4919-828e-f5720656be56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(stopwords.words('english')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22801303-48cb-4413-aba7-6e44264654f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa1549e8-58ca-4ba8-9bbc-fca8a2caeeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "tokens=[word for word in tokens if word not in string.punctuation]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6a875bf-b68c-43eb-9190-5ead47343de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "tokens=[word for word in tokens if word not in stopwords.words('english')]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55763d2d-38a0-49d3-bd8d-917e34372de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46865670-7c1c-4f55-9cc7-da384c96c907",
   "metadata": {},
   "source": [
    "# Stemming \n",
    "- reducing the word to their stem(base word) is called stemming.\n",
    "- ex- moving,move,moved - move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee92dc-a373-4340-ac63-1f11fcc7d31a",
   "metadata": {},
   "source": [
    "- It stems the word but sometimes the context of the word gets lost.\n",
    "- ex.- studies -> studi\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8dee35-31a2-489a-982c-d5601d811b4b",
   "metadata": {},
   "source": [
    "# About Lemmatization-\n",
    "- lemmatization is a concept which solves this problem.\n",
    "- and the base word will have some meaning.\n",
    "- lemmatization has a dictionary and will look for a word similiar to the stemmed word, and will eventually give the correct word which has that meaning\n",
    "- ex. - studies -> study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f58244-64e2-4b17-ab0d-2c5bdc168a2e",
   "metadata": {},
   "source": [
    "# Difference between stemming and lemmatization-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abece97-ad15-45e1-83d3-bacc033b1cfe",
   "metadata": {},
   "source": [
    "| Stemming | Lemmatization |\n",
    "| --- | --- |\n",
    "| - Stemming is faster. <br> - It is a rule-based approach (just remove the end part to stem the word). <br> - Less accuracy in stemming. <br> - Stemming is used in case of spam detection. | - Lemmatization is slower than stemming. <br> - It is a dictionary-based approach. <br> - Accuracy is more. <br> - Lemmatization is used for text summarizing, question and answers. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da355266-aac9-4048-b53d-a4dc0ed58025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54237b76-a85e-46af-9f2e-646d68640fa3",
   "metadata": {},
   "source": [
    "# Stemming-\n",
    "- Stemming is faster.\n",
    "- It is a rule based approach(just remove the end part to stem the word).\n",
    "- Less accuracy in stemming.\n",
    "- Stemming is used in case of spam detection.\n",
    "\n",
    "# Lemmatization-\n",
    "- Lemmatization is slower than stemming.\n",
    "- It is a dictionary based approach.\n",
    "- Accuracy is more.\n",
    "- Lemmatization is used for text summarizing, question and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d26f20a-de2d-4578-9619-10e1b1eb40aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
