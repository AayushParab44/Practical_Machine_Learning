{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc81fbce-c1ed-458d-bc51-2c7ed94da45d",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a34a4e-39c4-4ef7-bc24-9b2764a3de9e",
   "metadata": {},
   "source": [
    "The vertical split segregates - x and y where x is the predictors and y is the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d4023-42d1-4df4-b109-bf0aa0ff5909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "279c6c8d-e5a8-4270-a9e5-0aff70b00626",
   "metadata": {},
   "source": [
    "# Lexical Ambiguity - \n",
    "- word has different meanings.<br>\n",
    "- word being understood in more than 1 way.<br>\n",
    "- Machine has difficulty in interpreting these.\n",
    "- Ex.- Bank- 1. Financial institution, 2. River bank "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd421c-3ba8-42b7-9fbd-d90e376f374e",
   "metadata": {},
   "source": [
    "# Syntax Ambiguity - syntactic ambiguity\n",
    "- when a sentence can be parsed in different ways then this type of ambiguity is called syntactic ambiguity.<br>\n",
    "- ex. The man saw a girl with a telescope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194f687-89b1-4ccf-ad9f-07b740b1c8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fd979ca-63fe-4c65-adc0-de7a9a680c55",
   "metadata": {},
   "source": [
    "# Symantic ambiguity\n",
    "- when the meaning of the words themselves can be misinterpreted \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b871d-5b24-463a-9329-4774ac96e3d7",
   "metadata": {},
   "source": [
    "\n",
    "# Anaphoric ambiguity \n",
    "- arises due to the use of anaphore in the sentence\n",
    "- Ex- The horse run up the hill. It was too steep . It soon got tired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960caf01-9b5a-47d9-b974-cbec4b90896e",
   "metadata": {},
   "source": [
    "# Pragmatic ambiguity\n",
    "- When the statement is not specific or the context of the phrase gives multiple interpretations.\n",
    "- Ex. I love you too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a14502-9928-45f8-9076-54f5f068da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71c98c-27b2-4a4c-8a38-180759f12eb9",
   "metadata": {},
   "source": [
    "# Cleaning Data\n",
    "- convert data into lowercase\n",
    "- remove punctuations - .,,'',\"\",!,?,@,$,-\n",
    "- Remove stopwords - I, a ,an"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6efcd-746c-45e2-ba69-44108ceca865",
   "metadata": {},
   "source": [
    "- nltk library - natural language toolkit used in nlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7770c-431d-484c-ad1b-0d828bcbb913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e4ef3fc-4d6e-4112-87b1-9ad85de1ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import string\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64af8c5a-74e5-425e-9180-5a869cb91e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading nltk data(first time only)-\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger_eng') #supervised machine learning model for POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca000d-06f3-4174-a69a-317b504ac95a",
   "metadata": {},
   "source": [
    "# Tokenization \n",
    "- splitting a phrase/sentence/paragraph or entire text into smaller units(tokens) such as words is called as tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f97ac6b-d3d8-48ea-b47f-b24804711146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='The quick brown fox jumped over the lazy dog!'\n",
    "\n",
    "# Tokenize-\n",
    "tokens=word_tokenize(text.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a281091c-b8d0-452c-bb41-676e4b4d0f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "062e236e-645a-49ff-9625-e5bb58cef46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e1bc683-9965-4919-828e-f5720656be56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(stopwords.words('english')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22801303-48cb-4413-aba7-6e44264654f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa1549e8-58ca-4ba8-9bbc-fca8a2caeeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "tokens=[word for word in tokens if word not in string.punctuation]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6a875bf-b68c-43eb-9190-5ead47343de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "tokens=[word for word in tokens if word not in stopwords.words('english')]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55763d2d-38a0-49d3-bd8d-917e34372de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46865670-7c1c-4f55-9cc7-da384c96c907",
   "metadata": {},
   "source": [
    "# Stemming \n",
    "- reducing the word to their stem(base word) is called stemming.\n",
    "- ex- moving,move,moved - move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee92dc-a373-4340-ac63-1f11fcc7d31a",
   "metadata": {},
   "source": [
    "- It stems the word but sometimes the context of the word gets lost.\n",
    "- ex.- studies -> studi\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8dee35-31a2-489a-982c-d5601d811b4b",
   "metadata": {},
   "source": [
    "# About Lemmatization-\n",
    "- lemmatization is a concept which solves this problem.\n",
    "- and the base word will have some meaning.\n",
    "- lemmatization has a dictionary and will look for a word similiar to the stemmed word, and will eventually give the correct word which has that meaning\n",
    "- ex. - studies -> study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f58244-64e2-4b17-ab0d-2c5bdc168a2e",
   "metadata": {},
   "source": [
    "# Difference between stemming and lemmatization-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abece97-ad15-45e1-83d3-bacc033b1cfe",
   "metadata": {},
   "source": [
    "| Stemming | Lemmatization |\n",
    "| --- | --- |\n",
    "| - Stemming is faster. <br> - It is a rule-based approach (just remove the end part to stem the word). <br> - Less accuracy in stemming. <br> - Stemming is used in case of spam detection. | - Lemmatization is slower than stemming. <br> - It is a dictionary-based approach. <br> - Accuracy is more. <br> - Lemmatization is used for text summarizing, question and answers. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da355266-aac9-4048-b53d-a4dc0ed58025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54237b76-a85e-46af-9f2e-646d68640fa3",
   "metadata": {},
   "source": [
    "# Stemming-\n",
    "- Stemming is faster.\n",
    "- It is a rule based approach(just remove the end part to stem the word).\n",
    "- Less accuracy in stemming.\n",
    "- Stemming is used in case of spam detection.\n",
    "\n",
    "# Lemmatization-\n",
    "- Lemmatization is slower than stemming.\n",
    "- It is a dictionary based approach.\n",
    "- Accuracy is more.\n",
    "- Lemmatization is used for text summarizing, question and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0589b7-bb19-4c55-8627-2da7fbca3d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88a879e9-f263-43f1-99f4-dbff8dc71d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer  #PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "stemmed_words=[stemmer.stem(word) for word in tokens]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e7b66-ab41-417d-9e2a-b9bce0f401b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41451b77-915f-48a5-a0eb-6d7990efae4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()   #Lemmatizer\n",
    "tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7445f79a-2f4f-49bc-ba70-4af9fc07d7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0ddc3-996f-4e1c-a1f8-82353d8f5621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another example-\n",
    "# pos- parts of speech\n",
    "\n",
    "#Sample text\n",
    "text='Hello, All! This is an NLP example. I love learning about transformers, attention mechanisms, and embeddings'\n",
    "\n",
    "#step1: Tokenization:\n",
    "tokens=word_tokenize(text)\n",
    "print(\"Tokens:\",tokens)\n",
    "print('\\n')\n",
    "\n",
    "#step 2: lowercasing\n",
    "tokens=[word.lower() for word in tokens]\n",
    "print('Lowercased Tokens:',tokens)\n",
    "print('\\n')\n",
    "#step 3: Remove punctuation\n",
    "tokens=[word for word in tokens if word not in string.punctuation]\n",
    "print('Without punctuation:',tokens)\n",
    "print('\\n')\n",
    "\n",
    "#step 4: Remove stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "tokens=[word for word in tokens if word not in stop_words]\n",
    "print('Without stopwords:',tokens)\n",
    "print('\\n')\n",
    "\n",
    "#step 5 : Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
    "print('Lemmatized Tokens:',lemmatized_tokens)\n",
    "print('\\n')\n",
    "\n",
    "#step 6 : Stemming\n",
    "stemmer=PorterStemmer()\n",
    "stemmed_words=[stemmer.stem(word) for word in lemmatized_tokens]\n",
    "print('Stemmed tokens:',stemmed_words)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42ef6ebd-d6bf-43ba-ab8a-11881b06b0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'nlp',\n",
       " 'example',\n",
       " 'love',\n",
       " 'learning',\n",
       " 'transformers',\n",
       " 'attention',\n",
       " 'mechanisms',\n",
       " 'embeddings']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea81721-e12b-46f3-bf6f-16ed9865be28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Hello', ',', 'All', '!', 'This', 'is', 'an', 'NLP', 'example', '.', 'I', 'love', 'learning', 'about', 'transformers', ',', 'attention', 'mechanisms', ',', 'and', 'embeddings']\n",
      "\n",
      "\n",
      "Lowercased Tokens: ['hello', ',', 'all', '!', 'this', 'is', 'an', 'nlp', 'example', '.', 'i', 'love', 'learning', 'about', 'transformers', ',', 'attention', 'mechanisms', ',', 'and', 'embeddings']\n",
      "\n",
      "\n",
      "Without punctuation: ['hello', 'all', 'this', 'is', 'an', 'nlp', 'example', 'i', 'love', 'learning', 'about', 'transformers', 'attention', 'mechanisms', 'and', 'embeddings']\n",
      "\n",
      "\n",
      "Without stopwords: ['hello', 'nlp', 'example', 'love', 'learning', 'transformers', 'attention', 'mechanisms', 'embeddings']\n",
      "\n",
      "\n",
      "Lemmatized Tokens: ['hello', 'nlp', 'example', 'love', 'learning', 'transformer', 'attention', 'mechanism', 'embeddings']\n",
      "\n",
      "\n",
      "Stemmed tokens: ['hello', 'nlp', 'exampl', 'love', 'learn', 'transform', 'attent', 'mechan', 'embed']\n",
      "\n",
      "\n",
      "POS Tags: [('hello', 'NN'), ('nlp', 'JJ'), ('example', 'NN'), ('love', 'IN'), ('learning', 'VBG'), ('transformers', 'NNS'), ('attention', 'NN'), ('mechanisms', 'NNS'), ('embeddings', 'NNS')]\n",
      "\n",
      "\n",
      "Bigrams: [('hello', 'nlp'), ('nlp', 'example'), ('example', 'love'), ('love', 'learning'), ('learning', 'transformers'), ('transformers', 'attention'), ('attention', 'mechanisms'), ('mechanisms', 'embeddings')]\n"
     ]
    }
   ],
   "source": [
    "#step 7 : POS TAGGING-\n",
    "pos_tags=pos_tag(tokens)\n",
    "print('POS Tags:',pos_tags)\n",
    "print('\\n')\n",
    "\n",
    "#step 8: Generate N-grams\n",
    "bigrams=list(ngrams(tokens,2))\n",
    "print('Bigrams:',bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960094f-ac78-44d1-ad74-e779fa8c2b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8988f-8a60-406a-b1d2-51cf7fa5851a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be5b82a1-474b-435b-9fed-61a2f0ae892d",
   "metadata": {},
   "source": [
    "# POS tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53784c1-45f5-4f57-9245-7510af88de5b",
   "metadata": {},
   "source": [
    "pos_tag->meaning -> example\n",
    "1. NN ->Nouns singular->\n",
    "2. NNS-> Nouns Plural ->\n",
    "3. VB-> Verb base form-> run,eat,read\n",
    "4. VBD-> Verb in the past tense -> ate, ran,\n",
    "5. VBG -> ing form of verb -> running, sleeping\n",
    "6. VBN -> verb past participle -> eaten,\n",
    "7. JJ-> adjectives -> beautiful\n",
    "8. RB -> adverb -> slowly, quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5adb8cd4-0ac6-43ab-b002-5667f51d0e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordnet.NOUN\n",
    "# wordnet.VERB\n",
    "# wordnet.ADV\n",
    "# wordnet.ADJ\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521566f-3889-443e-ad96-cb8b21cceb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a94a5da1-1c2b-4bf2-aa63-575de483951f",
   "metadata": {},
   "source": [
    "# Bag of words -\n",
    "- is a text representation technique where the text is represented as collection of words(bag) where the grammar annd the word order is not taken into account but the frequency of the words matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96403a-3272-4654-a151-afa3d01a1c6c",
   "metadata": {},
   "source": [
    "Each sentence is considered as a document<br>\n",
    "vocabulary->unique words<br>\n",
    "- Tokenization\n",
    "- Vocabulary\n",
    "- Vectorisation(if a particualar word is there it is given value 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ed4106f-5417-4832-9210-5a785c726bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 6 stored elements and shape (2, 4)>\n",
      "  Coords\tValues\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 0)\t1\n",
      "[[0 1 1 1]\n",
      " [1 1 0 1]]\n",
      "['fun' 'learning' 'love' 'machine']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#sample documents\n",
    "docs=['I love machine learning','Machine learning is fun']\n",
    "\n",
    "#initialize count vectorizer\n",
    "vectorizer=CountVectorizer(lowercase=True,stop_words='english')\n",
    "\n",
    "#fit and transform the documents into a bag of words-\n",
    "X=vectorizer.fit_transform(docs)\n",
    "print(X)\n",
    "\n",
    "# convert the result to an array\n",
    "print(X.toarray())\n",
    "\n",
    "#Display the vocabulary(features)-\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b13aeb-2ef0-4eb3-8827-c591be4775a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d39d092-5b14-4d98-8975-bfd4f4201a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
